{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427af35f-2b34-48dd-aefc-b93266ca2f44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from transformers import (Trainer, TrainingArguments, DataCollatorWithPadding,\n",
    "                          AutoTokenizer, AutoModelForSequenceClassification, LongformerTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ac31a8-92cf-4eec-83a6-461f0a2e038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), 'GPU not found. You should fix this.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5afd7f-3b02-46f8-af40-827c2457ccb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = 'allenai/longformer-base-4096'\n",
    "dataset_path = '../bin/content_ds.hf'\n",
    "output_dir = 'results/hp-tuning'\n",
    "model_max_length = 2056\n",
    "eval_steps = 1000\n",
    "eval_accumulation_steps = 2\n",
    "save_total_limit = 4\n",
    "batch_size = 8\n",
    "sweep_id = None\n",
    "dry_run = False\n",
    "metric = 'mse'\n",
    "entity = 'ai-aloe'\n",
    "project_name = 'summary grader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28045349-1538-4eb5-bb46-eea4b673f84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    ds = DatasetDict.load_from_disk(dataset_path)\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a674f-22a2-4624-b7b5-98d0c69e3dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d89844-b2e7-410a-8944-1621e8e9b23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "\n",
    "    return {'mse': mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfcba266-364a-4db6-8436-52d4e663ed3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    ''' The main training loop.\n",
    "    '''\n",
    "    wandb.init()\n",
    "    \n",
    "    config = wandb.config\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            num_labels=1,\n",
    "            hidden_dropout_prob=config.dropout,\n",
    "        )\n",
    "        \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy='steps',\n",
    "        save_strategy='steps',\n",
    "        logging_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        save_steps=eval_steps,\n",
    "        eval_accumulation_steps=eval_accumulation_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        optim='adamw_torch',\n",
    "        gradient_accumulation_steps=4, \n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=config.learning_rate,\n",
    "        num_train_epochs=config.epochs,\n",
    "        weight_decay=config.weight_decay,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        load_best_model_at_end=False,\n",
    "        disable_tqdm=False,\n",
    "        report_to='wandb',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_dict['train'],\n",
    "        eval_dataset=dataset_dict['valid'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e86abb-e5f2-4430-8aeb-37f5acd05237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    max_length=model_max_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c81f79b-2a6f-4721-bad8-94114f36c709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: bdpy1h3m\n",
      "Sweep URL: https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wgog72l5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.14707222329850578\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.7627689085208615e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiedaar1\u001b[0m (\u001b[33mai-aloe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/active-projects/summary-scoring/src/wandb/run-20230521_225920-wgog72l5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai-aloe/summary%20grader/runs/wgog72l5' target=\"_blank\">apricot-sweep-1</a></strong> to <a href='https://wandb.ai/ai-aloe/summary%20grader' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai-aloe/summary%20grader' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai-aloe/summary%20grader/runs/wgog72l5' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/runs/wgog72l5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 2:11:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.98</td></tr><tr><td>train/global_step</td><td>306</td></tr><tr><td>train/total_flos</td><td>2.571161454128333e+16</td></tr><tr><td>train/train_loss</td><td>0.42086</td></tr><tr><td>train/train_runtime</td><td>7895.2788</td></tr><tr><td>train/train_samples_per_second</td><td>1.248</td></tr><tr><td>train/train_steps_per_second</td><td>0.039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-sweep-1</strong> at: <a href='https://wandb.ai/ai-aloe/summary%20grader/runs/wgog72l5' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/runs/wgog72l5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230521_225920-wgog72l5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e7p60a9t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.11333044665771856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.6966854486891115e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/active-projects/summary-scoring/src/wandb/run-20230522_011117-e7p60a9t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai-aloe/summary%20grader/runs/e7p60a9t' target=\"_blank\">gentle-sweep-2</a></strong> to <a href='https://wandb.ai/ai-aloe/summary%20grader' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai-aloe/summary%20grader' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai-aloe/summary%20grader/runs/e7p60a9t' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/runs/e7p60a9t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 2:11:33, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>2.98</td></tr><tr><td>train/global_step</td><td>306</td></tr><tr><td>train/total_flos</td><td>2.571161454128333e+16</td></tr><tr><td>train/train_loss</td><td>0.40477</td></tr><tr><td>train/train_runtime</td><td>7935.3666</td></tr><tr><td>train/train_samples_per_second</td><td>1.242</td></tr><tr><td>train/train_steps_per_second</td><td>0.039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-2</strong> at: <a href='https://wandb.ai/ai-aloe/summary%20grader/runs/e7p60a9t' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/runs/e7p60a9t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_011117-e7p60a9t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ix8i10ft with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.004479389147981206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.43362508169553e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/active-projects/summary-scoring/src/wandb/run-20230522_032353-ix8i10ft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai-aloe/summary%20grader/runs/ix8i10ft' target=\"_blank\">eager-sweep-3</a></strong> to <a href='https://wandb.ai/ai-aloe/summary%20grader' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai-aloe/summary%20grader' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/sweeps/bdpy1h3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai-aloe/summary%20grader/runs/ix8i10ft' target=\"_blank\">https://wandb.ai/ai-aloe/summary%20grader/runs/ix8i10ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17/204 06:32 < 1:21:32, 0.04 it/s, Epoch 0.16/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict.load_from_disk(dataset_path)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=16, return_tensors='pt')\n",
    "\n",
    "if not sweep_id:\n",
    "    sweep_goal = 'minimize'\n",
    "        \n",
    "    if dry_run:\n",
    "        sweep_name = 'dry-run'\n",
    "    else:\n",
    "        sweep_name = f'{model_name_or_path}'\n",
    "            \n",
    "    sweep_config = {\n",
    "        'name': sweep_name,\n",
    "        'method': 'bayes',\n",
    "        'metric': {\n",
    "            'name': f'eval/{metric}',\n",
    "            'goal': sweep_goal,\n",
    "        },\n",
    "        'parameters':\n",
    "        {\n",
    "            'epochs': {\n",
    "                'values': [2, 3]\n",
    "            },\n",
    "            'dropout': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0,\n",
    "                'max': 0.2\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 1e-5,\n",
    "                'max': 2e-5,\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'values': [0.3]\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config,\n",
    "                            entity=entity,\n",
    "                            project=project_name)\n",
    "\n",
    "else:\n",
    "    sweep_id = sweep_id\n",
    "        \n",
    "wandb.agent(sweep_id, train, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cb1c6-bc79-414c-9115-8e5efb02b461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4b7a9-aba7-49e9-9232-b081227de90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:WesleyEnv]",
   "language": "python",
   "name": "conda-env-WesleyEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
