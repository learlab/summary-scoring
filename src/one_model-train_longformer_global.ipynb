{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0322552a-fcd1-4dbe-99e1-2b9f6c35d496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric, Dataset, Value, ClassLabel, Features, DatasetDict\n",
    "import torch\n",
    "\n",
    "sns.set_theme(font='Liberation Serif',\n",
    "              rc={'figure.figsize': (7.5,3.75),\n",
    "                  'font.size': 11,\n",
    "                  'figure.dpi': 300,\n",
    "                 })\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70067938-5424-4fb4-b44a-9d58989752a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b2cc0c-87fb-4ea2-bb7d-47ddf4cc2b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA = '../data/'\n",
    "DATAFRAME = 'final_summaries_ai_aloe_fixed.csv'\n",
    "summaries_df = pd.read_csv(DATA + DATAFRAME)\n",
    "texts_to_remove = ['Q31', 'Q35', 'Q41', 'Q39', 'Q33', 'Red Blood Cells SE Task', 'GW_C', 'GW_A', 'GW_B', \n",
    "                   'GW_D', '14_ComputerVirus', '5_FireworksDanger', '23_InternetShopping', '6_Smoking', \n",
    "                   '10_Internet', 'Low Air Fares_Money']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9273e88-e569-4d2a-ab63-e042faee04ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataframe(summaries_df):\n",
    "    # copy the data\n",
    "    df_normalized = summaries_df.copy()  \n",
    "    # apply normalization techniques\n",
    "    df_normalized['content_pca'] = StandardScaler().fit_transform(np.array(df_normalized['content_pca']).reshape(-1,1))\n",
    "    df_normalized['paraphrase_pca'] = StandardScaler().fit_transform(np.array(df_normalized['paraphrase_pca']).reshape(-1,1))\n",
    "    # combine labels into a single vector\n",
    "    df_normalized['labels'] = df_normalized.apply(lambda row: [row['content_pca'], row['paraphrase_pca']], axis=1)\n",
    "    # combine source and summary\n",
    "    df_normalized['text'] = df_normalized['text'] + '</s>' + df_normalized['source']\n",
    "    # remove \\r tokens\n",
    "    df_normalized['text'] = df_normalized['text'].str.replace('\\xa0', '')\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594b2dc4-10c5-457b-ab98-66417505d7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def buildDataset(df):\n",
    "    full_dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "    # 70% train, 30% test\n",
    "    train_valid = full_dataset.train_test_split(test_size=0.176, seed=seed)\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    final_dataset = DatasetDict({\n",
    "        'train': train_valid['train'],\n",
    "        'valid': train_valid['test']})\n",
    "    return final_dataset\n",
    "\n",
    "def prepare_dataset(df_normalized, texts_to_remove):\n",
    "    test_df = df_normalized[df_normalized['source_text_filename_clean'].isin(texts_to_remove)][['text', 'labels']]\n",
    "    test_df.columns = ['text', 'labels']\n",
    "    train_df = df_normalized[df_normalized['source_text_filename_clean'].isin(texts_to_remove) == False][['text', 'labels']]\n",
    "    test_df.columns = ['text', 'labels']    \n",
    "    ds = buildDataset(train_df)\n",
    "    ds['test'] = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db72d7e-a99f-454a-bf36-a4fe337161e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = prepare_dataset(prepare_dataframe(summaries_df), texts_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46548ed1-d04d-4b29-bb7e-c0af1b74da85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, LongformerConfig, logging\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name, padding=True, truncation=True)\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda19e03-e4bb-464d-8f43-acc0d032e5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_inputs(example):\n",
    "    return tokenizer(example['text'], truncation = True, padding='max_length')\n",
    "\n",
    "def custom_global_attention(example):\n",
    "    try:\n",
    "        sep_index = example['input_ids'].index(2)\n",
    "        global_attention_mask = [1]*(sep_index + 1) + [0]*(len(example['input_ids'])-(sep_index + 1))\n",
    "        return {'global_attention_mask': global_attention_mask}\n",
    "    except:\n",
    "        print(example['input_ids'], example['text'])\n",
    "\n",
    "def tokenize_dataset(ds):\n",
    "    ds1 = ds.map(tokenize_inputs, batched=True)\n",
    "    ds2 = ds1.map(custom_global_attention, batched=False)\n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d0cd5a-f2b5-4d17-b7a2-0c11afc9b4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "ds_t = tokenize_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "598df117-af64-4306-9024-747c7539eb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()  \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.hidden_dropout_prob = 0\n",
    "        self.config.attention_probs_dropout_prob = 0\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, len(CONFIG['label_cols']))\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids,\n",
    "                         attention_mask=attention_mask, \n",
    "                         output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, attention_mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return SequenceClassifierOutput(logits=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "677996d8-05c2-4952-98f8-36f5c08a7538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Code taken from Y Nakama's notebook (https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = torch.sqrt(self.mse(predictions, targets) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        loss_func = RMSELoss(reduction='mean')\n",
    "        loss = loss_func(outputs.logits.float(), inputs['labels'].float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    colwise_rmse = np.sqrt(np.mean((labels - predictions) ** 2, axis=0))\n",
    "    res = {\n",
    "        f\"{analytic.upper()}_RMSE\" : colwise_rmse[i]\n",
    "        for i, analytic in enumerate(model.config.label_cols)\n",
    "    }\n",
    "    res[\"MCRMSE\"] = np.mean(colwise_rmse)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2182a-2124-44e0-96da-c358dab16ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "257984a7-d21d-42ba-9801-b8f49977394f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 3e-05\n",
    "batch_size = 8\n",
    "seed = 42\n",
    "num_epochs = 10\n",
    "\n",
    "def model_init():\n",
    "    model = LongformerForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    model.config.attention_window = [256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256]\n",
    "    model.config.label_cols = ['content', 'wording']\n",
    "    model.config.loss_type = \"rmse\"\n",
    "    print(model.config)\n",
    "    return model\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a5bdc2d-0b6f-478c-bbf5-357f089c5653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask', 'global_attention_mask'],\n",
       "        num_rows: 3285\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask', 'global_attention_mask'],\n",
       "        num_rows: 702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask', 'global_attention_mask'],\n",
       "        num_rows: 703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e841d73-883a-407d-8b27-67e2a805dbad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label_cols\": [\n",
      "    \"content\",\n",
      "    \"wording\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"loss_type\": \"rmse\",\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label_cols\": [\n",
      "    \"content\",\n",
      "    \"wording\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"loss_type\": \"rmse\",\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label_cols\": [\n",
      "    \"content\",\n",
      "    \"wording\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"loss_type\": \"rmse\",\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1020 6:31:22, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Content Rmse</th>\n",
       "      <th>Wording Rmse</th>\n",
       "      <th>Mcrmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.613957</td>\n",
       "      <td>0.817851</td>\n",
       "      <td>0.840009</td>\n",
       "      <td>0.828930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.443717</td>\n",
       "      <td>0.563365</td>\n",
       "      <td>0.623571</td>\n",
       "      <td>0.593468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.408237</td>\n",
       "      <td>0.504193</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.543650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.407543</td>\n",
       "      <td>0.502683</td>\n",
       "      <td>0.593058</td>\n",
       "      <td>0.547871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.404073</td>\n",
       "      <td>0.494443</td>\n",
       "      <td>0.571877</td>\n",
       "      <td>0.533160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.377127</td>\n",
       "      <td>0.468664</td>\n",
       "      <td>0.536159</td>\n",
       "      <td>0.502411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.388321</td>\n",
       "      <td>0.483676</td>\n",
       "      <td>0.557724</td>\n",
       "      <td>0.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.376389</td>\n",
       "      <td>0.468830</td>\n",
       "      <td>0.546936</td>\n",
       "      <td>0.507883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.377610</td>\n",
       "      <td>0.467427</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>0.505431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.375735</td>\n",
       "      <td>0.466987</td>\n",
       "      <td>0.543408</td>\n",
       "      <td>0.505198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2372: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1020, training_loss=0.37509298838821115, metrics={'train_runtime': 23504.4792, 'train_samples_per_second': 1.398, 'train_steps_per_second': 0.043, 'total_flos': 8.56876064427049e+16, 'train_loss': 0.37509298838821115, 'epoch': 9.93})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f'./results/longformer_one-mod_checkpoints',\n",
    "    optim = 'adamw_torch',\n",
    "    num_train_epochs = num_epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    weight_decay = 0.01,\n",
    "    learning_rate = learning_rate,\n",
    "    gradient_accumulation_steps=4, \n",
    "    gradient_checkpointing=True,\n",
    "    logging_dir = f'./logs/content',\n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'eval_MCRMSE',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\", \n",
    "    greater_is_better = False,\n",
    "    seed=seed,\n",
    "    disable_tqdm = False, \n",
    ") \n",
    "\n",
    "    # Call the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model_init = model_init,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = ds_t['train'],\n",
    "    eval_dataset = ds_t['valid'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1b1f16f-9aa7-4620-89ff-f480c9bf12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('../bin/one_model_longformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0f533d4-91ed-4818-a289-99a91a93c31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10dee1f7-e5ef-4340-802b-20834c9e5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerForSequenceClassification.from_pretrained('../bin/one_model_longformer', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d88abca-6835-4e05-81fd-0139f73cd83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not Dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_t\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_envs/WesleyEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda_envs/WesleyEnv/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py:1936\u001b[0m, in \u001b[0;36mLongformerForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing global attention on CLS token...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1936\u001b[0m     global_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;66;03m# global attention on cls token\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m     global_attention_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros_like(): argument 'input' (position 1) must be Tensor, not Dataset"
     ]
    }
   ],
   "source": [
    "model(ds_t['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:WesleyEnv]",
   "language": "python",
   "name": "conda-env-WesleyEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
