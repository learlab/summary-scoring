{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3d55394-deb1-48f0-8884-621645f9adb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongformerForSequenceClassification, pipeline\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd910e14-d2ef-4f2b-a21a-c97befe3f178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = \"Here is an essay about economics. It is really interesting and I like to write about economics. Economics is the best in the whole world ever.\"\n",
    "source = \"Economics is the study of money. Money is fun but also scary. /n Economics is fun!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1054ac2b-0839-41f5-afc4-2903f96fbbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "wording_model = LongformerForSequenceClassification.from_pretrained('tiedaar/longformer-wording-global', num_labels=1)\n",
    "content_model = LongformerForSequenceClassification.from_pretrained('tiedaar/longformer-content-global', num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116b117a-b2f1-4123-8ecb-066130779d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(summary, source, model):\n",
    "    combined = summary + '</s>' + source\n",
    "    context = tokenizer(combined)\n",
    "    sep_index = context['input_ids'].index(2)\n",
    "    context['global_attention_mask'] = [1]*(sep_index + 1) + [0]*(len(context['input_ids'])-(sep_index + 1))\n",
    "    inputs = {}\n",
    "    for key in context:\n",
    "        inputs[key] = torch.tensor([context[key]])\n",
    "    return float(model(**inputs)['logits'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bebef-defc-4781-b60e-a0c573ba81f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f450805d-65a2-4462-8371-5d85de51d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test n: 703\n",
      "train n: 3987\n"
     ]
    }
   ],
   "source": [
    "DATA = '../data/'\n",
    "summaries_df = pd.read_csv(DATA + 'final_summaries_ai_aloe_fixed.csv').drop(columns = ['Unnamed: 0','Unnamed: 0.1'])\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "  \n",
    "# copy the data\n",
    "df_normalized = summaries_df.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "df_normalized['content_pca'] = StandardScaler().fit_transform(np.array(df_normalized['content_pca']).reshape(-1,1))\n",
    "df_normalized['paraphrase_pca'] = StandardScaler().fit_transform(np.array(df_normalized['paraphrase_pca']).reshape(-1,1))  \n",
    "\n",
    "\n",
    "source_texts = df_normalized['source_text_filename_clean'].value_counts().to_frame().reset_index()\n",
    "texts_to_remove = list(source_texts.iloc[15:31]['source_text_filename_clean'])\n",
    "\n",
    "test_df = df_normalized[df_normalized['source_text_filename_clean'].isin(texts_to_remove)]\n",
    "train_df = df_normalized[df_normalized['source_text_filename_clean'].isin(texts_to_remove) == False]\n",
    "print('test n:', len(test_df))\n",
    "print('train n:', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20202281-0f2c-421c-bdcd-188b51474e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3605/2965409399.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['content_global_pred'] = test_df.apply(lambda row: inference(row['text'], row['source'], content_model), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test_df['content_global_pred'] = test_df.apply(lambda row: inference(row['text'], row['source'], content_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "038a4292-1f55-4446-9bd8-59ad9029e296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3605/4215338332.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['wording_global_pred'] = test_df.apply(lambda row: inference(row['text'], row['source'], wording_model), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test_df['wording_global_pred'] = test_df.apply(lambda row: inference(row['text'], row['source'], wording_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df7ec57e-e71b-4f61-a849-9c7223a4ef88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tiedaar/summary-longformer-wording were not used when initializing LongformerForSequenceClassification: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at tiedaar/summary-longformer-content were not used when initializing LongformerForSequenceClassification: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "wording_pipe = pipeline('text-classification', model='tiedaar/summary-longformer-wording', function_to_apply=\"none\", truncation=True)\n",
    "content_pipe = pipeline('text-classification', model='tiedaar/summary-longformer-content', function_to_apply=\"none\", truncation=True)\n",
    "def getWordingScore(summary, source):\n",
    "    text = summary + '</s>' + source\n",
    "    return wording_pipe(text)[0]['score']\n",
    "\n",
    "def getContentScore(summary, source):\n",
    "    text = summary + '</s>' + source\n",
    "    return content_pipe(text)[0]['score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f829b8-3aa6-43b6-88b6-3e0fb828b2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3605/2852758028.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['content_pred'] = test_df.apply(lambda row: getContentScore(row['text'], row['source']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test_df['content_pred'] = test_df.apply(lambda row: getContentScore(row['text'], row['source']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd7e75fd-5010-4a2c-857e-7c4e2df2191c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3605/1518824042.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['wording_pred'] = test_df.apply(lambda row: getWordingScore(row['text'], row['source']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test_df['wording_pred'] = test_df.apply(lambda row: getWordingScore(row['text'], row['source']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d47a6a-2e80-4390-95f2-a8b515ba5a85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_pca</th>\n",
       "      <th>paraphrase_pca</th>\n",
       "      <th>content_pred</th>\n",
       "      <th>wording_pred</th>\n",
       "      <th>content_global_pred</th>\n",
       "      <th>wording_global_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>content_pca</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710421</td>\n",
       "      <td>0.881808</td>\n",
       "      <td>0.737226</td>\n",
       "      <td>0.907282</td>\n",
       "      <td>0.681890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paraphrase_pca</th>\n",
       "      <td>0.710421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670423</td>\n",
       "      <td>0.822417</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.836284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_pred</th>\n",
       "      <td>0.881808</td>\n",
       "      <td>0.670423</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.793873</td>\n",
       "      <td>0.944221</td>\n",
       "      <td>0.723264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wording_pred</th>\n",
       "      <td>0.737226</td>\n",
       "      <td>0.822417</td>\n",
       "      <td>0.793873</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785188</td>\n",
       "      <td>0.917834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content_global_pred</th>\n",
       "      <td>0.907282</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.944221</td>\n",
       "      <td>0.785188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wording_global_pred</th>\n",
       "      <td>0.681890</td>\n",
       "      <td>0.836284</td>\n",
       "      <td>0.723264</td>\n",
       "      <td>0.917834</td>\n",
       "      <td>0.741664</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     content_pca  paraphrase_pca  content_pred  wording_pred   \n",
       "content_pca             1.000000        0.710421      0.881808      0.737226  \\\n",
       "paraphrase_pca          0.710421        1.000000      0.670423      0.822417   \n",
       "content_pred            0.881808        0.670423      1.000000      0.793873   \n",
       "wording_pred            0.737226        0.822417      0.793873      1.000000   \n",
       "content_global_pred     0.907282        0.696000      0.944221      0.785188   \n",
       "wording_global_pred     0.681890        0.836284      0.723264      0.917834   \n",
       "\n",
       "                     content_global_pred  wording_global_pred  \n",
       "content_pca                     0.907282             0.681890  \n",
       "paraphrase_pca                  0.696000             0.836284  \n",
       "content_pred                    0.944221             0.723264  \n",
       "wording_pred                    0.785188             0.917834  \n",
       "content_global_pred             1.000000             0.741664  \n",
       "wording_global_pred             0.741664             1.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['content_pca', 'paraphrase_pca', 'content_pred', 'wording_pred', 'content_global_pred', 'wording_global_pred']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa54418-d27d-485f-84b5-4eb6a7e0c720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:WesleyEnv]",
   "language": "python",
   "name": "conda-env-WesleyEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
